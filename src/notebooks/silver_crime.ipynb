{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef2282c-c045-4134-8c5f-c3f5304606e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ============ 配置 ============\n",
    "env = \"dev\"\n",
    "catalog = f\"{env}_catalog\"\n",
    "schema = f\"crime_data_{env}\"\n",
    "\n",
    "source_table = f\"{catalog}.{schema}.bronze_crime\"\n",
    "target_table = f\"{catalog}.{schema}.silver_crime\"\n",
    "checkpoint_path = f\"abfss://{env}@kevintestdatabricks.dfs.core.windows.net/_checkpoints/silver_crime/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547bb5fe-03df-42c9-9adf-bf652072e2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============ 创建目标表（如果不存在）============\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_table} (\n",
    "    id STRING,\n",
    "    case_number STRING,\n",
    "    date STRING,\n",
    "    block STRING,\n",
    "    iucr STRING,\n",
    "    primary_type STRING,\n",
    "    description STRING,\n",
    "    location_description STRING,\n",
    "    arrest BOOLEAN,\n",
    "    domestic BOOLEAN,\n",
    "    beat STRING,\n",
    "    district STRING,\n",
    "    ward STRING,\n",
    "    community_area STRING,\n",
    "    fbi_code STRING,\n",
    "    x_coordinate FLOAT,\n",
    "    y_coordinate FLOAT,\n",
    "    year STRING,\n",
    "    updated_on STRING,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    location STRING,\n",
    "    _ingestion_time TIMESTAMP,\n",
    "    _source_file STRING,\n",
    "    _env STRING,\n",
    "    _silver_processed_time TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (\n",
    "    'delta.enableChangeDataFeed' = 'true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✅ Target table ready: {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d76338f-db28-4674-8b30-84b7cd60ed6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============ 定义 MERGE 函数 ============\n",
    "def upsert_to_silver(batch_df, batch_id):\n",
    "    \"\"\"增量合并到 Silver 表，基于 id 去重\"\"\"\n",
    "    \n",
    "    # 类型转换\n",
    "    transformed_df = (batch_df\n",
    "        .withColumn(\"latitude\", col(\"latitude\").cast(\"float\"))\n",
    "        .withColumn(\"longitude\", col(\"longitude\").cast(\"float\"))\n",
    "        .withColumn(\"x_coordinate\", col(\"x_coordinate\").cast(\"float\"))\n",
    "        .withColumn(\"y_coordinate\", col(\"y_coordinate\").cast(\"float\"))\n",
    "        .withColumn(\"_silver_processed_time\", current_timestamp())\n",
    "        # 批次内去重（保留最新）\n",
    "        .dropDuplicates([\"id\"])\n",
    "    )\n",
    "    \n",
    "    # 如果目标表存在，执行 MERGE\n",
    "    if spark.catalog.tableExists(target_table):\n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        \n",
    "        (delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                transformed_df.alias(\"source\"),\n",
    "                \"target.id = source.id\"\n",
    "            )\n",
    "            .whenMatchedUpdate(\n",
    "                condition=\"source.updated_on > target.updated_on\",  # 只更新更新时间更晚的\n",
    "                set={\n",
    "                    \"case_number\": \"source.case_number\",\n",
    "                    \"date\": \"source.date\",\n",
    "                    \"block\": \"source.block\",\n",
    "                    \"iucr\": \"source.iucr\",\n",
    "                    \"primary_type\": \"source.primary_type\",\n",
    "                    \"description\": \"source.description\",\n",
    "                    \"location_description\": \"source.location_description\",\n",
    "                    \"arrest\": \"source.arrest\",\n",
    "                    \"domestic\": \"source.domestic\",\n",
    "                    \"beat\": \"source.beat\",\n",
    "                    \"district\": \"source.district\",\n",
    "                    \"ward\": \"source.ward\",\n",
    "                    \"community_area\": \"source.community_area\",\n",
    "                    \"fbi_code\": \"source.fbi_code\",\n",
    "                    \"x_coordinate\": \"source.x_coordinate\",\n",
    "                    \"y_coordinate\": \"source.y_coordinate\",\n",
    "                    \"year\": \"source.year\",\n",
    "                    \"updated_on\": \"source.updated_on\",\n",
    "                    \"latitude\": \"source.latitude\",\n",
    "                    \"longitude\": \"source.longitude\",\n",
    "                    \"location\": \"source.location\",\n",
    "                    \"_ingestion_time\": \"source._ingestion_time\",\n",
    "                    \"_source_file\": \"source._source_file\",\n",
    "                    \"_env\": \"source._env\",\n",
    "                    \"_silver_processed_time\": \"source._silver_processed_time\"\n",
    "                }\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    else:\n",
    "        # 表不存在，直接写入\n",
    "        transformed_df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "    \n",
    "    print(f\"✅ Batch {batch_id}: Processed {transformed_df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2f1953-3f1e-4c45-8f3d-7662f050aa03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============ 从 Bronze 增量读取并处理 ============\n",
    "# 使用 readStream 从 Delta 表增量读取\n",
    "bronze_stream = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreChanges\", \"true\")  # 忽略更新，只读新增\n",
    "    .table(source_table)\n",
    ")\n",
    "\n",
    "# 使用 foreachBatch 执行 MERGE\n",
    "query = (bronze_stream.writeStream\n",
    "    .foreachBatch(upsert_to_silver)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .trigger(availableNow=True)  # 处理完当前数据就停止\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"✅ Silver table updated: {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c41d306-24ab-4bc0-8829-9f7e4b895e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============ 验证结果 ============\n",
    "print(f\"Bronze count: {spark.table(source_table).count()}\")\n",
    "print(f\"Silver count: {spark.table(target_table).count()}\")\n",
    "\n",
    "# 检查是否有重复 ID\n",
    "duplicate_check = spark.sql(f\"\"\"\n",
    "    SELECT id, COUNT(*) as cnt \n",
    "    FROM {target_table} \n",
    "    GROUP BY id \n",
    "    HAVING cnt > 1\n",
    "\"\"\")\n",
    "print(f\"Duplicate IDs: {duplicate_check.count()}\")\n",
    "\n",
    "# 查看数据\n",
    "display(spark.table(target_table).limit(10))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_crime",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
